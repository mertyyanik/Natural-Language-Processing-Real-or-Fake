{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Tensorflow2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertyyanik/Natural-Language-Processing-Real-or-Fake/blob/master/News_Tensorflow2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftLLUHP9IAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "74f5525b-f280-457b-ce77-c84536c59a9a"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        " \n",
        "from pydrive.drive import GoogleDrive\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "auth.authenticate_user()\n",
        " \n",
        "gauth = GoogleAuth()\n",
        " \n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 2.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.6MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbCZOgTqXKNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Test.csv Dosyası\n",
        "\n",
        "file_id = '1Zx7P0t9lQLP4aj4YOUaZDxNRrSkZOTk4'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "#Test.txt Dosyası\n",
        " \n",
        "file_id = '1S2-N2HSIgtZpSzPmAEZWmVxV2YlOzCzs'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('test.txt')\n",
        "\n",
        "#clean_real-Train.txt Dosyası\n",
        " \n",
        "file_id = '1pnsf7izWzNYlA41xcWrCXQ5SSFe5JCUF'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_real-Train.txt')\n",
        "\n",
        "#clean_fake-Train.txt Dosyası\n",
        " \n",
        "file_id = '1Mc1m6S6WCJuRkMBWM9_LZGHSzJIw2-e0'\n",
        " \n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        " \n",
        "downloaded.GetContentFile('clean_fake-Train.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIuR9igwXNLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTEcrFW5XQ91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Data\n",
        "input1Data = open('clean_real-Train.txt', 'r+')\n",
        "input2Data = open('clean_fake-Train.txt', 'r+')\n",
        "testData = open('test.txt')\n",
        "\n",
        "testDataForComparison = pd.read_csv('test.csv').iloc[:, 1:]\n",
        "\n",
        "input1List = []\n",
        "input2List = []\n",
        "testList = []\n",
        "\n",
        "read = input1Data.readline()\n",
        "read2 = input2Data.readline()\n",
        "read3 = testData.readline()\n",
        "rowCount = 0\n",
        "while read != \"\":\n",
        "    input1List.append(read)\n",
        "    read = input1Data.readline()\n",
        "    \n",
        "while read2 != \"\":\n",
        "    input2List.append(read2)\n",
        "    read2 = input2Data.readline()\n",
        "\n",
        "while read3 != \"\":\n",
        "    testList.append(read3)\n",
        "    read3 = testData.readline()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWW1jtXmXUV_",
        "colab_type": "code",
        "outputId": "fb3bd8c8-a915-4898-f256-59d09b428645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Real and Fake data combined.    \n",
        "liste = ['real' for i in range(len(input1List))]\n",
        "realInput = np.column_stack((input1List, liste))\n",
        "\n",
        "\n",
        "liste = ['fake' for i in range(len(input2List))]\n",
        "fakeInput = np.column_stack((input2List, liste))\n",
        "\n",
        "mainInput = realInput.tolist() + fakeInput.tolist()\n",
        "random.shuffle(mainInput)\n",
        "\n",
        "sentenceInput = []\n",
        "targetInput = []\n",
        "targetInputFloat = []\n",
        "for i in range(len(mainInput)):\n",
        "    sentenceInput.append(mainInput[i][0])\n",
        "    \n",
        "for i in range(len(mainInput)):\n",
        "    targetInput.append(mainInput[i][1])\n",
        "    \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "targetInputFloat = lb.fit_transform(targetInput)\n",
        "testDataForComparison = lb.fit_transform(testDataForComparison)\n",
        "\n",
        "targetInputFloat = pd.DataFrame(data=targetInputFloat)\n",
        "testDataForComparison = pd.DataFrame(data = testDataForComparison)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hAeq9bGXYRj",
        "colab_type": "code",
        "outputId": "f75261ff-cca5-4064-9360-afbd5aad5a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Stop Words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "sentenceList = []\n",
        "for i in range(len(sentenceInput)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', sentenceInput[i]) #-> 1.Adım\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower() #-> 2. Adım\n",
        "    # Splitting\n",
        "    comment = comment.split() #-> 3.Adım\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))] # -> 4. Adım\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    sentenceList.append(comment)\n",
        "    \n",
        "temporaryResult = pd.DataFrame(data=sentenceList)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj00WRu2XbiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorized All Data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentenceList)\n",
        "\n",
        "result = pd.DataFrame(data=X.toarray())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzl7PQWDXfVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Data\n",
        "testSentenceList = []\n",
        "for i in range(len(testList)):\n",
        "    # Regular Expressions\n",
        "    comment = re.sub('[^a-zA-Z]', ' ', testList[i])\n",
        "    # Upper Case Problem\n",
        "    comment = comment.lower()\n",
        "    # Splitting\n",
        "    comment = comment.split()\n",
        "    # Stemming and Stop Words\n",
        "    comment = [ps.stem(kelime) for kelime in comment if not kelime in set(stopwords.words('english'))]\n",
        "    \n",
        "    comment = ' '.join(comment)\n",
        "    testSentenceList.append(comment)\n",
        "\n",
        "X = vectorizer.transform(testSentenceList)\n",
        "\n",
        "testResult = pd.DataFrame(data=X.toarray())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skKqAaejXjL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg-Cq_VaZqgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(x, y, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  #dataset = dataset.shuffle(len(x))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "train_set = create_dataset(result.values, targetInputFloat.values, len(result))\n",
        "test_set = create_dataset(testResult.values, testDataForComparison.values, len(testResult))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JbrPOpd-Yv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features, labels = next(iter(train_set))\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP9dnCOvX3ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                         tf.keras.layers.Dense(1500, kernel_initializer='uniform', activation=tf.nn.relu, input_dim=3854),\n",
        "                         tf.keras.layers.Dropout(0.5),\n",
        "                         tf.keras.layers.Dense(1500, kernel_initializer='uniform', activation=tf.nn.relu),\n",
        "                         tf.keras.layers.Dropout(0.5),\n",
        "                         tf.keras.layers.Dense(1, kernel_initializer = 'uniform', activation =tf.nn.sigmoid),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrSIypukgZEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(model, observation, label):\n",
        "  prediction = model(observation, training = True)\n",
        "  loss = tf.keras.losses.binary_crossentropy(label, prediction)\n",
        "  return loss\n",
        "\n",
        "for gozlem, etiket in train_set.take(1):\n",
        "  print(f\"Gerçeğe olan uzaklık: {loss_function(model, gozlem, etiket)}\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q41-eVngjzFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3zpRFkij29e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_process(model, observation, label):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_function(model, observation, label)\n",
        "        \n",
        "    gradients_of_model = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients_of_model, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC6upQr5kenC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "  for x, y in train_set:\n",
        "    loss_value = gradient_process(model, x, y)\n",
        "\n",
        "    epoch_loss_avg(loss_value)  \n",
        "    \n",
        "  if epoch % 2 == 0:\n",
        "    print(f'Epoch {epoch}: Loss: {epoch_loss_avg.result()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2IQqSCiw2sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "prediction = []\n",
        "for x, y in test_set:\n",
        "  prediction.append(model.predict(x))\n",
        "\n",
        "prediction = np.asarray(prediction)\n",
        "\n",
        "prediction = (prediction > 0.5)\n",
        "\n",
        "prediction = prediction.reshape(489,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDS21O8RGzQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acScoreNN = accuracy_score(testDataForComparison, prediction)\n",
        "print(\"Accuracy : \" + str(acScoreNN))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}